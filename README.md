# ğŸ¤– Memory-Enhanced Personalized Chatbot

A smart chatbot that remembers who you are! This project uses Google Gemini API, FastAPI, Redis, and Streamlit to create a personalized chat experience with both **short-term memory** (chat history) and **long-term memory** (stored facts about the user).

---

## ğŸ“Œ Features

- ğŸ§  **Memory-aware assistant** â€” stores and recalls user details like name, location, preferences, projects.
- ğŸ” **Short-term memory** â€” remembers recent conversation context (last 5â€“10 messages).
- ğŸ’¾ **Long-term memory** â€” uses Redis to persist important facts.
- âš¡ **FastAPI backend** â€” serves a structured Gemini-powered LLM response.
- ğŸ¯ **Streamlit frontend** â€” clean, responsive user interface.
- ğŸ› ï¸ **Fact extraction** â€” structured personal facts are categorized and stored.

---

## ğŸ§  How It Works

### ğŸ”¹ Short-Term Memory
Stored in memory as recent messages. Injected into prompt to keep conversational context.

### ğŸ”¹ Long-Term Memory (Redis)
Facts are extracted by Gemini from conversations and stored under:

```
user:<user_id>:memory:<category>
```

Categories include:
- `profile`: Name, location, age, etc.
- `projects`: Userâ€™s work or academic projects
- `preferences`: Likes, dislikes, tools used
- `general`: Unstructured facts

---

## ğŸ’¾ Redis Memory View

To inspect what's stored:

```bash
redis-cli
> keys user:user_124:memory:*
> smembers user:user_124:memory:profile
```

---

## ğŸ“ Project Structure

```
personalized-chatbot/
â”‚
â”œâ”€â”€ main.py              # FastAPI backend with Gemini & memory logic
â”œâ”€â”€ chat_history.py      # Stores recent messages (short-term)
â”œâ”€â”€ memory.py            # Redis interactions (long-term memory)
â”œâ”€â”€ categorize.py        # Determines memory category
â”œâ”€â”€ config.py            # API keys and model config
â”œâ”€â”€ streamlit_app.py     # Streamlit UI
â”œâ”€â”€ requirements.txt     # All dependencies
â””â”€â”€ README.md            # This file
```

---

## ğŸ› ï¸ Installation & Setup

### 1. Clone the Repo

```bash
git clone https://github.com/your-username/personalized-chatbot.git
cd personalized-chatbot
```

### 2. Create and Activate Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

### 3. Install Requirements

```bash
pip install -r requirements.txt
```

### 4. Set Up Your Gemini API Key

Create `.env` file and add:

```python
GEMINI_API_KEY = "your-api-key"
```

### 5. Start Redis Server

If Redis is not installed:

```bash
# Ubuntu
sudo apt update
sudo apt install redis
redis-server
```

Or use Docker:

```bash
docker run -d -p 6379:6379 redis
```

---

## ğŸš€ Run the Application

### Start Backend (FastAPI):

```bash
uvicorn main:app --reload
```

### Start Frontend (Streamlit):

```bash
streamlit run streamlit_app.py
```

---

## ğŸ§ª Sample Interaction

```text
You: I'm Fasi from Lahore.
Bot: Nice to meet you, Fasi! Lahore is a beautiful city.

Facts extracted:
- Name: Fasi
- Location: Lahore
```

These are stored in Redis and used in future chats.

---

## ğŸ§© Reducing Repetition

The backend avoids duplicating stored facts by:
- Retrieving existing facts
- Checking if a fact already exists before saving (optional improvement you can add)

---

## â±ï¸ Measuring Latency

Each request's latency is printed in milliseconds in the backend logs using:

```python
start = time.perf_counter()
...
end = time.perf_counter()
print(f"Latency: {round((end - start) * 1000, 2)} ms")
```

---

## ğŸ”® Future Enhancements

- Semantic memory search with FAISS or LightRAG
- Edit/delete memory entries from UI
- Real-time message streaming
- User authentication
- Support for OpenAI / Claude / Cohere

---

## ğŸ“„ License

MIT License

---

## ğŸ™Œ Acknowledgements

- [Google Gemini API](https://ai.google.dev/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [Redis](https://redis.io/)
- [Streamlit](https://streamlit.io/)

---

## ğŸ‘¨â€ğŸ’» Author

Fasi Tahir â€“ [@FasiTahir](https://github.com/FasiTahir)
